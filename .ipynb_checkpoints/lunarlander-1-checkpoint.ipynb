{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAM306 - Land a rocket (and a good game) with reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition\n",
    "\n",
    "Use this notebook to train your lunar lander agent for the competition! This notebook is the same as the tutorial notebook you went through, so everything you learned there applies here. The only thing that is different is now you can set which scenario you want to work on. This will be covered in Step 1. There are two notebooks named `lunarlander-1.ipynb` and `lunarlander-2.ipynb` so that you and your teammate can work at the same time on two different scenarios if you choose to do so. Divide and conquer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Scenarios\n",
    "\n",
    "These coefficients represent various weights that alter the reward function to fit each scenario listed below. Since the goal for each scenario is different, there needs to be a different reward function so the agent can learn appropriately.\n",
    "\n",
    "### Open AI gym default - tutorial\n",
    "c1 = c2 = c3 = 100; c4= 0.3\n",
    "\n",
    "### Softest landing\n",
    "c1 = 500\n",
    "            \n",
    "### Most centered landing\n",
    "c2 = 500   \n",
    "            \n",
    "### Most level landing\n",
    "c3 = 500            \n",
    "            \n",
    "### Minimum fuel\n",
    "c4 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Choose your scenario\n",
    "\n",
    "Choose which scenario you want to work on by setting the _**scenario**_ variable in the next code cell to the appropriate number. For example, if you want to train for scenario 1 - softest landing, set `scenario ='1'`. The competition only counts for scenarios 1-4.\n",
    "\n",
    "1. Set `scenaro = '1'` below to do the softest landing scenario. Or, set this value to whatever scenario you want to work on. \n",
    "\n",
    "2. Run the code cell below to set the scenario variable by hitting the **Run** button in the toolbar above.\n",
    "\n",
    "Make sure to re-run the code cell below to reset the scenario variable every time you want to change the scenario you are working on throughout this workshop.  \n",
    "\n",
    "This code cell first defines that we are using the [Lunar Lander](https://gym.openai.com/envs/LunarLander-v2/) problem in the [Box2D](https://gym.openai.com/envs/#box2d) environment from [OpenAI Gym](https://openai.com/). \n",
    "\n",
    "It then sets the training algorithm to **PPO** which stands for Proximal Policy Optimization. This is a new class of reinforcement learning algorithms that is easy to use and offers good performance. You can read more about PPO [here](https://openai.com/blog/openai-baselines-ppo/#ppo).\n",
    "\n",
    "Finally, it imports necessary files for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "box2d_problem = 'lunarlander'\n",
    "\n",
    "# Algorithm \n",
    "algo = 'PPO'\n",
    "\n",
    "#Choose scenario 0, 1, 2, 3 or 4\n",
    "#Scenario 0 - Tutorial\n",
    "#Scenario 1 - Softest landing\n",
    "#Scenario 2 - Accurate landing\n",
    "#Scenario 3 - Level landing\n",
    "#Scenario 4 - Minimal fuel usage\n",
    "#Rerun cell to be able to set a different scenario \n",
    "scenario='1'\n",
    "\n",
    "trainscript = 'train-{}-{}.py'.format(box2d_problem,algo)\n",
    "\n",
    "!cp src/lunar_lander-{scenario}.py src/lunar_lander.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train-lunarlander-PPO.py'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainscript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### States\n",
    "\n",
    "What are the states of this agent? This is a sample that shows how the state of the lunar lander agent is determined. It contains information about the state of the agent at every step of the process, including the position of the agent for example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "pos = self.lander.position\n",
    "        vel = self.lander.linearVelocity\n",
    "        state = [\n",
    "            (pos.x - VIEWPORT_W/SCALE/2) / (VIEWPORT_W/SCALE/2),\n",
    "            (pos.y - (self.helipad_y+LEG_DOWN/SCALE)) / (VIEWPORT_H/SCALE/2),\n",
    "            vel.x*(VIEWPORT_W/SCALE/2)/FPS,\n",
    "            vel.y*(VIEWPORT_H/SCALE/2)/FPS,\n",
    "            self.lander.angle,\n",
    "            20.0*self.lander.angularVelocity/FPS,\n",
    "            1.0 if self.legs[0].ground_contact else 0.0,\n",
    "            1.0 if self.legs[1].ground_contact else 0.0\n",
    "]\n",
    "```       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions\n",
    "\n",
    "What are the actions the agent can take? For example, the agent might be able to take no action. Or maybe the agent fires the left engine, or the main engine, or the right engine. \n",
    "\n",
    "According to Pontryagin's maximum principle, it is optimal to fire the engine full throttle or turn it off. That's the reason this environment is OK to have discreet actions (engine on or off)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python        \n",
    "        if self.continuous:\n",
    "            # Action is two floats [main engine, left-right engines].\n",
    "            # Main engine: -1..0 off, 0..+1 throttle from 50% to 100% power. Engine can't work with less than 50% power.\n",
    "            # Left-right:  -1.0..-0.5 fire left engine, +0.5..+1.0 fire right engine, -0.5..0.5 off\n",
    "            self.action_space = spaces.Box(-1, +1, (2,), dtype=np.float32)\n",
    "        else:\n",
    "            # Nop, fire left engine, main engine, right engine\n",
    "            self.action_space = spaces.Discrete(4)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward\n",
    "\n",
    "How is the agent rewarded? This is a sample reward function for the lunar lander agent. In this function, you can see that the reward increases or decreases depending on the state of the lunar lander agent. For example, if you want a level landing, the reward increases for each leg that contacts with the moon. This is to train the lunar lander to land right-side up instead of upside down. To a similar effect, the reward decreases depending on how much fuel is used. This is to train the lunar lander to conserve as much fuel as possible. \n",
    "\n",
    "The reward is officially calculated as follows:\n",
    "\n",
    "The landing pad is always at coordinates (0,0). The coordinates are the first two numbers in the state vector shown below. The reward for moving from the top of the screen to the landing pad with zero speed is about 100-140 points.\n",
    "If the lander moves away from landing pad, it loses reward. The episode finishes if the lander crashes or\n",
    "comes to a rest, receiving an additional -100 or +100 points respectively. Each leg ground contact is +10. Firing the main engine is -0.3 points each frame. Firing the side engine is -0.03 points each frame. Solving the lunar lander is 200 points.\n",
    "\n",
    "Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land\n",
    "on its first attempt. You can see the source code for more details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "        reward = 0\n",
    "        shaping = \\\n",
    "            - 100*np.sqrt(state[0]*state[0] + state[1]*state[1]) \\\n",
    "            - 100*np.sqrt(state[2]*state[2] + state[3]*state[3]) \\\n",
    "            - 100*abs(state[4]) + 10*state[6] + 10*state[7]\n",
    "        # Add ten points for legs contact, the idea is if you\n",
    "        # lose contact again after landing, you get negative reward\n",
    "        if self.prev_shaping is not None:\n",
    "            reward = shaping - self.prev_shaping\n",
    "        self.prev_shaping = shaping\n",
    "\n",
    "        reward -= m_power*0.30  # less fuel spent is better, about -30 for heurisic landing\n",
    "        reward -= s_power*0.03\n",
    "\n",
    "        done = False\n",
    "        if self.game_over or abs(state[0]) >= 1.0:\n",
    "            done   = True\n",
    "            reward = -100\n",
    "        if not self.lander.awake:\n",
    "            done   = True\n",
    "            reward = +100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Set prerequisites \n",
    "\n",
    "### Imports\n",
    "\n",
    "To get started, we'll import the Python libraries we need and set up the environment with a few prerequisites for permissions and configurations. This code cell imports necessary Python libraries, like **boto3** which is the AWS SDK for Python.\n",
    "\n",
    "3. Run the code cell below to set the prerequisites needed for this workshop by hitting the **Run** button in the toolbar above.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "sys.path.append(\"common\")\n",
    "from misc import get_execution_role, wait_for_s3_object\n",
    "from docker_utils import build_and_push_docker_image\n",
    "from sagemaker.rl import RLEstimator, RLToolkit, RLFramework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Setup S3 bucket\n",
    "\n",
    "Next, we need to set up the linkage and authentication to an S3 bucket. This will be the bucket where SageMaker stores the output data of training jobs. SageMaker also stores the trained models as **model.tar.gz** files in this S3 bucket, as well as checkpoints and other metadata. \n",
    "\n",
    "4. Run the code cell below by hitting the **Run** button in the toolbar above.\n",
    "\n",
    "This code cell creates a SageMaker session, which helps to manage interactions with the Amazon SageMaker APIs and any other AWS services needed, like S3. The S3 bucket is set to the default SageMaker bucket and the output path of this bucket is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 bucket path: s3://sagemaker-us-east-1-793689376757/\n"
     ]
    }
   ],
   "source": [
    "sage_session = sagemaker.session.Session()\n",
    "s3_bucket = sage_session.default_bucket()  \n",
    "s3_output_path = 's3://{}/'.format(s3_bucket)\n",
    "print(\"S3 bucket path: {}\".format(s3_output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Define variables and configure training\n",
    "\n",
    "We define variables such as the job prefix for the training jobs *and the image path for the container (only when this is BYOC).*\n",
    "\n",
    "5. **Run** the code cell below to set the job name. **DO NOT EDIT THIS CELL AT ALL!** Just run the cell as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a descriptive job name \n",
    "job_name_prefix = scenario + '-rl-box2d-'+box2d_problem+scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure where training happens\n",
    "\n",
    "You can train your RL training jobs using the SageMaker notebook instance or local notebook instance. In both of these scenarios, you can run the following in either local or SageMaker modes. The local mode uses the SageMaker Python SDK to run your code in a local container before deploying to SageMaker. This can speed up iterative testing and debugging while using the same familiar Python SDK interface. You just need to set `local_mode = True`.\n",
    "\n",
    "6. For now, leave this cell default with local_mode set to **False**.\n",
    "\n",
    "7. **Run** the code cell.\n",
    "\n",
    "**Tip:** In the future, you can fire off multiple training jobs on different instance types by changing the instance type below before training. Right now, the instance type is **ml.p3.16xlarge**. For this workshop, you have the ability to use the follow instance types for distributed training:\n",
    "\n",
    "* (2) ml.p3.2xlarge instances\n",
    "\n",
    "* (2) ml.p3.8xlarge instances\n",
    "\n",
    "* (2) ml.p3.16xlarge instances\n",
    "\n",
    "You also have access to any of the [default SageMaker limits](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html#limits_sagemaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in local_mode on this machine, or as a SageMaker TrainingJob?\n",
    "local_mode = False\n",
    "\n",
    "if local_mode:\n",
    "    instance_type = 'local'\n",
    "else:\n",
    "    # If on SageMaker, pick the instance type\n",
    "    instance_type = \"ml.p3.2xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an IAM role\n",
    "\n",
    "Either get the execution role when running from a SageMaker notebook instance `role = sagemaker.get_execution_role()` or, when running from local notebook instance, use utils method `role = get_execution_role()` to create an execution role.\n",
    "\n",
    "8. **Run** this code cell to ceate an IAM role for SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using IAM role arn: arn:aws:iam::793689376757:role/mod-2a01731acb2241e7-SagemakerIAM-17T4W130FWHO2\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except:\n",
    "    role = get_execution_role()\n",
    "\n",
    "print(\"Using IAM role arn: {}\".format(role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install docker for `local` mode\n",
    "\n",
    "In order to work in `local` mode, you need to have docker installed. When running from you local machine, please make sure that you have docker and docker-compose (for local CPU machines) and nvidia-docker (for local GPU machines) installed. Alternatively, when running from a SageMaker notebook instance, you can simply run the following script to install dependenceis.\n",
    "\n",
    "9. **Run** the code cell below. \n",
    "\n",
    "**Tip:** You can only run a single local notebook at one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run from SageMaker notebook instance\n",
    "if local_mode:\n",
    "    !/bin/bash ./common/setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Build docker container\n",
    "\n",
    "We must build a custom docker container with Roboschool installed.  This takes care of everything:\n",
    "\n",
    "* Fetching base container image\n",
    "* Installing Roboschool and its dependencies\n",
    "* Uploading the new container image to ECR\n",
    "\n",
    "This step can take a long time if you are running on a machine with a slow internet connection.  If your notebook instance is in SageMaker or EC2 it should take 3-10 minutes depending on the instance type.\n",
    "\n",
    "10. **Run** the code cell to build the docker container. \n",
    "\n",
    "**Tip:** The output should say `Done pushing` when the build is finished. If your build seems to be hung up on something, either not progressing or looks like it is not running at all, refresh your notebook and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "Logged into ECR\n",
      "Building docker image lunarlander-gpu from Dockerfile\n",
      "$ docker build -t lunarlander-gpu -f Dockerfile . --build-arg CPU_OR_GPU=gpu --build-arg AWS_REGION=us-east-1\n",
      "Sending build context to Docker daemon  726.5kB\n",
      "Step 1/31 : ARG CPU_OR_GPU\n",
      "Step 2/31 : ARG AWS_REGION\n",
      "Step 3/31 : FROM 520713654638.dkr.ecr.${AWS_REGION}.amazonaws.com/sagemaker-rl-tensorflow:ray0.6.5-${CPU_OR_GPU}-py3\n",
      " ---> f038be2cc8c4\n",
      "Step 4/31 : WORKDIR /opt/ml\n",
      " ---> Using cache\n",
      " ---> e6f5ed3314ff\n",
      "Step 5/31 : RUN apt-get update\n",
      " ---> Using cache\n",
      " ---> 647d06cbfc46\n",
      "Step 6/31 : RUN apt-get install sudo\n",
      " ---> Using cache\n",
      " ---> 5be1835b8d58\n",
      "Step 7/31 : RUN apt-get update && apt-get install -y --no-install-recommends apt-utils\n",
      " ---> Using cache\n",
      " ---> 3832846bf4ba\n",
      "Step 8/31 : RUN apt-get -y install libpcre3\n",
      " ---> Using cache\n",
      " ---> 31bc4a6206af\n",
      "Step 9/31 : RUN apt-get -y install libpcre3-dev\n",
      " ---> Using cache\n",
      " ---> 8843100ab3c9\n",
      "Step 10/31 : ARG SWIG_VERSION=4.0.1\n",
      " ---> Using cache\n",
      " ---> 75a50c2c6473\n",
      "Step 11/31 : RUN wget \"https://sourceforge.net/projects/swig/files/swig/swig-${SWIG_VERSION}/swig-${SWIG_VERSION}.tar.gz\"     && tar xzf \"swig-${SWIG_VERSION}.tar.gz\"\n",
      " ---> Using cache\n",
      " ---> ef3c4706436c\n",
      "Step 12/31 : RUN cd \"swig-${SWIG_VERSION}/\"     && ./configure     && make     && make install\n",
      " ---> Using cache\n",
      " ---> 435cfdcfae6c\n",
      "Step 13/31 : CMD [\"swig\",\"-version\"]\n",
      " ---> Using cache\n",
      " ---> 0e0157b19d0d\n",
      "Step 14/31 : RUN apt-get update && apt-get install -y       git cmake ffmpeg pkg-config       qtbase5-dev libqt5opengl5-dev libassimp-dev       libtinyxml-dev       libgl1-mesa-dev     && cd /opt     && apt-get clean && rm -rf /var/cache/apt/archives/* /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 79c3e4d34d3c\n",
      "Step 15/31 : RUN apt-get update &&     apt-get install -y libboost-python-dev\n",
      " ---> Using cache\n",
      " ---> e5532b978368\n",
      "Step 16/31 : RUN apt-get update     && apt-get install -y --no-install-recommends python3.6-dev     && ln -s -f /usr/bin/python3.6 /usr/bin/python     && apt-get clean     && rm -rf /var/cache/apt/archives/* /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 3e4e2c85ab6a\n",
      "Step 17/31 : RUN curl -fSsL -O https://bootstrap.pypa.io/get-pip.py &&     python get-pip.py &&     rm get-pip.py\n",
      " ---> Using cache\n",
      " ---> 596b99c051f6\n",
      "Step 18/31 : RUN pip install --upgrade     pip     setuptools\n",
      " ---> Using cache\n",
      " ---> 3fd30b64dcb7\n",
      "Step 19/31 : RUN pip install sagemaker-containers --upgrade\n",
      " ---> Using cache\n",
      " ---> 7252a3b9f6e6\n",
      "Step 20/31 : RUN pip install roboschool==1.0.46\n",
      " ---> Using cache\n",
      " ---> 740c28262553\n",
      "Step 21/31 : RUN yes y | pip uninstall gym; exit 0\n",
      " ---> Using cache\n",
      " ---> e9964abf2dfe\n",
      "Step 22/31 : RUN pip install box2d-py\n",
      " ---> Using cache\n",
      " ---> 50a5c6e25c5e\n",
      "Step 23/31 : RUN pip install gym[box2d]; exit 0\n",
      " ---> Using cache\n",
      " ---> dbb3bd19627f\n",
      "Step 24/31 : ENV PYTHONUNBUFFERED 1\n",
      " ---> Using cache\n",
      " ---> f454a1db32c7\n",
      "Step 25/31 : RUN pip show gym\n",
      " ---> Using cache\n",
      " ---> b1056380991b\n",
      "Step 26/31 : RUN echo $SCENARIO\n",
      " ---> Using cache\n",
      " ---> daf3c6502116\n",
      "Step 27/31 : COPY ./src/lunar_lander.py /usr/local/lib/python3.6/dist-packages/gym/envs/box2d/lunar_lander.py\n",
      " ---> 7ccc58c45e39\n",
      "Step 28/31 : RUN echo -----------------------------------------------\n",
      " ---> Running in b3aa81fd775d\n",
      "-----------------------------------------------\n",
      "Removing intermediate container b3aa81fd775d\n",
      " ---> e745bc072bdf\n",
      "Step 29/31 : RUN ls /usr/local/lib/python3.6/dist-packages/gym/envs/box2d/\n",
      " ---> Running in 22a525d6e175\n",
      "__init__.py\n",
      "__pycache__\n",
      "bipedal_walker.py\n",
      "car_dynamics.py\n",
      "car_racing.py\n",
      "lunar_lander.py\n",
      "test_lunar_lander.py\n",
      "Removing intermediate container 22a525d6e175\n",
      " ---> 9c18c2b6d325\n",
      "Step 30/31 : RUN python -c \"import gym;import sagemaker_containers.cli.train;import roboschool; import ray; from sagemaker_containers.cli.train import main; import Box2D;\"\n",
      " ---> Running in 42ac7e03d3a1\n",
      "Removing intermediate container 42ac7e03d3a1\n",
      " ---> 20a902a380d4\n",
      "Step 31/31 : WORKDIR /opt/ml/code\n",
      " ---> Running in 9c05a111468a\n",
      "Removing intermediate container 9c05a111468a\n",
      " ---> 6887e820ccf0\n",
      "Successfully built 6887e820ccf0\n",
      "Successfully tagged lunarlander-gpu:latest\n",
      "Done building docker image lunarlander-gpu\n",
      "ECR repository already exists: lunarlander-gpu\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "Logged into ECR\n",
      "$ docker tag lunarlander-gpu 793689376757.dkr.ecr.us-east-1.amazonaws.com/lunarlander-gpu\n",
      "Pushing docker image to ECR repository 793689376757.dkr.ecr.us-east-1.amazonaws.com/lunarlander-gpu\n",
      "\n",
      "$ docker push 793689376757.dkr.ecr.us-east-1.amazonaws.com/lunarlander-gpu\n",
      "The push refers to repository [793689376757.dkr.ecr.us-east-1.amazonaws.com/lunarlander-gpu]\n",
      "f79108a48ae1: Preparing\n",
      "498265fb044f: Preparing\n",
      "d4424ee2a91d: Preparing\n",
      "c9ee9a5f1f8e: Preparing\n",
      "c32729b069d3: Preparing\n",
      "e5a0d8d622ed: Preparing\n",
      "75771a874b6b: Preparing\n",
      "0855fd75afe3: Preparing\n",
      "de2d09cc8d31: Preparing\n",
      "b7edc8fc6235: Preparing\n",
      "ffb55a7b92d1: Preparing\n",
      "05e4ebb8463a: Preparing\n",
      "d03ff26b69d6: Preparing\n",
      "0b0aa35f003c: Preparing\n",
      "50328b3418db: Preparing\n",
      "0d75372ce2bb: Preparing\n",
      "22226c0d1fe6: Preparing\n",
      "fb03cd5fdcf6: Preparing\n",
      "8e168c4e2b95: Preparing\n",
      "91290f0ea2df: Preparing\n",
      "977d98febf7c: Preparing\n",
      "bac377b479f5: Preparing\n",
      "5f529b3bfb80: Preparing\n",
      "46fead19683e: Preparing\n",
      "fb13b553d663: Preparing\n",
      "9edb700225ef: Preparing\n",
      "a097915916bf: Preparing\n",
      "7e6a109ce771: Preparing\n",
      "b2dfc5305f98: Preparing\n",
      "1e6ccf08dae4: Preparing\n",
      "8394491e531a: Preparing\n",
      "a191b6a85373: Preparing\n",
      "cdb84aebd22c: Preparing\n",
      "df9b6d412b8f: Preparing\n",
      "9fdd75ed2314: Preparing\n",
      "e7deb83143c4: Preparing\n",
      "b4f24c5badc2: Preparing\n",
      "3048b56d4fe2: Preparing\n",
      "8dd047faddca: Preparing\n",
      "cadf36f9989b: Preparing\n",
      "03da9992e0d0: Preparing\n",
      "0b3aca7c75ea: Preparing\n",
      "297fd071ca2f: Preparing\n",
      "2f0d1e8214b2: Preparing\n",
      "7dd604ffa87f: Preparing\n",
      "aa54c2bc1229: Preparing\n",
      "e5a0d8d622ed: Waiting\n",
      "75771a874b6b: Waiting\n",
      "0855fd75afe3: Waiting\n",
      "de2d09cc8d31: Waiting\n",
      "b7edc8fc6235: Waiting\n",
      "ffb55a7b92d1: Waiting\n",
      "05e4ebb8463a: Waiting\n",
      "d03ff26b69d6: Waiting\n",
      "0b0aa35f003c: Waiting\n",
      "50328b3418db: Waiting\n",
      "0d75372ce2bb: Waiting\n",
      "22226c0d1fe6: Waiting\n",
      "fb03cd5fdcf6: Waiting\n",
      "8e168c4e2b95: Waiting\n",
      "91290f0ea2df: Waiting\n",
      "977d98febf7c: Waiting\n",
      "bac377b479f5: Waiting\n",
      "5f529b3bfb80: Waiting\n",
      "46fead19683e: Waiting\n",
      "fb13b553d663: Waiting\n",
      "9edb700225ef: Waiting\n",
      "a097915916bf: Waiting\n",
      "7e6a109ce771: Waiting\n",
      "b2dfc5305f98: Waiting\n",
      "1e6ccf08dae4: Waiting\n",
      "8394491e531a: Waiting\n",
      "a191b6a85373: Waiting\n",
      "cdb84aebd22c: Waiting\n",
      "df9b6d412b8f: Waiting\n",
      "9fdd75ed2314: Waiting\n",
      "e7deb83143c4: Waiting\n",
      "b4f24c5badc2: Waiting\n",
      "3048b56d4fe2: Waiting\n",
      "8dd047faddca: Waiting\n",
      "cadf36f9989b: Waiting\n",
      "03da9992e0d0: Waiting\n",
      "0b3aca7c75ea: Waiting\n",
      "297fd071ca2f: Waiting\n",
      "2f0d1e8214b2: Waiting\n",
      "7dd604ffa87f: Waiting\n",
      "aa54c2bc1229: Waiting\n",
      "d4424ee2a91d: Layer already exists\n",
      "c32729b069d3: Layer already exists\n",
      "c9ee9a5f1f8e: Layer already exists\n",
      "e5a0d8d622ed: Layer already exists\n",
      "75771a874b6b: Layer already exists\n",
      "0855fd75afe3: Layer already exists\n",
      "de2d09cc8d31: Layer already exists\n",
      "b7edc8fc6235: Layer already exists\n",
      "ffb55a7b92d1: Layer already exists\n",
      "05e4ebb8463a: Layer already exists\n",
      "d03ff26b69d6: Layer already exists\n",
      "50328b3418db: Layer already exists\n",
      "0d75372ce2bb: Layer already exists\n",
      "0b0aa35f003c: Layer already exists\n",
      "22226c0d1fe6: Layer already exists\n",
      "8e168c4e2b95: Layer already exists\n",
      "fb03cd5fdcf6: Layer already exists\n",
      "91290f0ea2df: Layer already exists\n",
      "977d98febf7c: Layer already exists\n",
      "bac377b479f5: Layer already exists\n",
      "5f529b3bfb80: Layer already exists\n",
      "9edb700225ef: Layer already exists\n",
      "46fead19683e: Layer already exists\n",
      "fb13b553d663: Layer already exists\n",
      "7e6a109ce771: Layer already exists\n",
      "b2dfc5305f98: Layer already exists\n",
      "a097915916bf: Layer already exists\n",
      "1e6ccf08dae4: Layer already exists\n",
      "a191b6a85373: Layer already exists\n",
      "8394491e531a: Layer already exists\n",
      "cdb84aebd22c: Layer already exists\n",
      "f79108a48ae1: Pushed\n",
      "df9b6d412b8f: Layer already exists\n",
      "498265fb044f: Pushed\n",
      "9fdd75ed2314: Layer already exists\n",
      "e7deb83143c4: Layer already exists\n",
      "b4f24c5badc2: Layer already exists\n",
      "3048b56d4fe2: Layer already exists\n",
      "cadf36f9989b: Layer already exists\n",
      "03da9992e0d0: Layer already exists\n",
      "8dd047faddca: Layer already exists\n",
      "0b3aca7c75ea: Layer already exists\n",
      "2f0d1e8214b2: Layer already exists\n",
      "7dd604ffa87f: Layer already exists\n",
      "297fd071ca2f: Layer already exists\n",
      "aa54c2bc1229: Layer already exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest: digest: sha256:ce5c897820bec29633e6d7b5f614b2933f52fefc752a698c2fdd47753729ac58 size: 9976\n",
      "Done pushing 793689376757.dkr.ecr.us-east-1.amazonaws.com/lunarlander-gpu\n",
      "Using ECR image 793689376757.dkr.ecr.us-east-1.amazonaws.com/lunarlander-gpu\n",
      "CPU times: user 130 ms, sys: 40.6 ms, total: 170 ms\n",
      "Wall time: 6.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cpu_or_gpu = 'gpu' if instance_type.startswith('ml.p') else 'cpu'\n",
    "repository_short_name = box2d_problem+\"-%s\" % cpu_or_gpu\n",
    "docker_build_args = {\n",
    "    'CPU_OR_GPU': cpu_or_gpu, \n",
    "    'AWS_REGION': boto3.Session().region_name\n",
    "}\n",
    "custom_image_name = build_and_push_docker_image(repository_short_name, build_args=docker_build_args)\n",
    "print(\"Using ECR image %s\" % custom_image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'793689376757.dkr.ecr.us-east-1.amazonaws.com/lunarlander-gpu'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_image_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Write the Training Code\n",
    "\n",
    "The training code is written in the file “train-coach.py” which is uploaded in the /src directory. \n",
    "First import the environment files and the preset files, and then define the main() function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mgym\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mray\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mray.tune\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m run_experiments\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mray.tune.registry\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m register_env\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mroboschool\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_rl.ray_launcher\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SageMakerRayLauncher\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcreate_environment\u001b[39;49;00m(env_config):\r\n",
      "    \u001b[37m# This import must happen inside the method so that worker processes import this code\u001b[39;49;00m\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mroboschool\u001b[39;49;00m\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m gym.make(\u001b[33m'\u001b[39;49;00m\u001b[33mLunarLander-v2\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mMyLauncher\u001b[39;49;00m(SageMakerRayLauncher):\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mregister_env_creator\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        register_env(\u001b[33m\"\u001b[39;49;00m\u001b[33mLunarLander-v2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, create_environment)\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mget_experiment_config\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m {\r\n",
      "          \u001b[33m\"\u001b[39;49;00m\u001b[33mtraining\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: { \r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33menv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mLunarLander-v2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mrun\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mPPO\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mstop\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: {\r\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mepisode_reward_mean\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m0\u001b[39;49;00m,\r\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mtraining_iteration\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m50\u001b[39;49;00m,\r\n",
      "            },\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mconfig\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: {\r\n",
      "              \u001b[33m\"\u001b[39;49;00m\u001b[33mnum_sgd_iter\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m330\u001b[39;49;00m,\r\n",
      "              \u001b[33m\"\u001b[39;49;00m\u001b[33mlr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m5e-5\u001b[39;49;00m,\r\n",
      "              \u001b[33m\"\u001b[39;49;00m\u001b[33mlambda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[34m0.9998201362028988\u001b[39;49;00m,\r\n",
      "              \u001b[33m\"\u001b[39;49;00m\u001b[33mvf_loss_coeff\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[34m0.9624532314793395\u001b[39;49;00m,\r\n",
      "              \u001b[33m\"\u001b[39;49;00m\u001b[33mkl_target\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[34m0.05500572866402965\u001b[39;49;00m,\r\n",
      "              \u001b[33m\"\u001b[39;49;00m\u001b[33mkl_coeff\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[34m0.22702622027852548\u001b[39;49;00m,\r\n",
      "              \u001b[33m\"\u001b[39;49;00m\u001b[33mentropy_coeff\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[34m0.4906464378184655\u001b[39;49;00m,\r\n",
      "              \u001b[33m\"\u001b[39;49;00m\u001b[33mclip_param\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[34m0.745631860794536\u001b[39;49;00m,\r\n",
      "              \u001b[33m\"\u001b[39;49;00m\u001b[33msgd_minibatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m5000\u001b[39;49;00m,\r\n",
      "              \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m25000\u001b[39;49;00m,\r\n",
      "              \u001b[33m\"\u001b[39;49;00m\u001b[33mmonitor\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[36mTrue\u001b[39;49;00m,  \u001b[37m# Record videos.\u001b[39;49;00m\r\n",
      "              \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: {\r\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mfree_log_std\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[36mTrue\u001b[39;49;00m\r\n",
      "              },\r\n",
      "              \u001b[33m\"\u001b[39;49;00m\u001b[33mnum_workers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[36mself\u001b[39;49;00m.num_cpus-\u001b[34m1\u001b[39;49;00m),\r\n",
      "              \u001b[33m\"\u001b[39;49;00m\u001b[33mnum_gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[36mself\u001b[39;49;00m.num_gpus,\r\n",
      "              \u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_mode\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mcomplete_episodes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "            }\r\n",
      "          }\r\n",
      "        }\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "    MyLauncher().train_main()\r\n",
      "    \r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/{trainscript}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "To tune hyperparameters, find the `train-lunarlander-PPO.py` file in the `src` folder of your SageMaker notebook. You can tune hyperparameters by editing the values in the `def get_experiment_config(self):` function. Below is a descripttion of what the hyperparameters are.\n",
    "\n",
    "11. Research the hyperparameters below to understand how changing them affects the lunar lander agent. Tune the hyperparameters in the `train-lunarlander-PPO.py` file to create the most optimal RL agent.\n",
    "\n",
    "Please see https://arxiv.org/abs/1707.06347 and https://arxiv.org/pdf/1506.02438.pdf for details \n",
    "\n",
    "\n",
    "### DEFAULT_CONFIG\n",
    "    # If true, use the Generalized Advantage Estimator (GAE)\n",
    "    # with a value function, see .\n",
    "    \"use_gae\": True,\n",
    "    # GAE(lambda) parameter\n",
    "    \"lambda\": 1.0,\n",
    "    # Initial coefficient for KL divergence\n",
    "    \"kl_coeff\": 0.2,\n",
    "    # Size of batches collected from each worker\n",
    "    \"sample_batch_size\": 200,\n",
    "    # Number of timesteps collected for each SGD round\n",
    "    \"train_batch_size\": 4000,\n",
    "    # Total SGD batch size across all devices for SGD\n",
    "    \"sgd_minibatch_size\": 128,\n",
    "    # Whether to shuffle sequences in the batch when training (recommended)\n",
    "    \"shuffle_sequences\": True,\n",
    "    # Number of SGD iterations in each outer loop\n",
    "    \"num_sgd_iter\": 30,\n",
    "    # Stepsize of SGD\n",
    "    \"lr\": 5e-5,\n",
    "    # Learning rate schedule\n",
    "    \"lr_schedule\": None,\n",
    "    # Share layers for value function. If you set this to True, it's important\n",
    "    # to tune vf_loss_coeff.\n",
    "    \"vf_share_layers\": False,\n",
    "    # Coefficient of the value function loss. It's important to tune this if\n",
    "    # you set vf_share_layers: True\n",
    "    \"vf_loss_coeff\": 1.0,\n",
    "    # Coefficient of the entropy regularizer\n",
    "    \"entropy_coeff\": 0.0,\n",
    "    # Decay schedule for the entropy regularizer\n",
    "    \"entropy_coeff_schedule\": None,\n",
    "    # PPO clip parameter\n",
    "    \"clip_param\": 0.3,\n",
    "    # Clip param for the value function. Note that this is sensitive to the\n",
    "    # scale of the rewards. If your expected V is large, increase this.\n",
    "    \"vf_clip_param\": 10.0,\n",
    "    # If specified, clip the global norm of gradients by this amount\n",
    "    \"grad_clip\": None,\n",
    "    # Target value for KL divergence\n",
    "    \"kl_target\": 0.01,\n",
    "    # Whether to rollout \"complete_episodes\" or \"truncate_episodes\"\n",
    "    \"batch_mode\": \"truncate_episodes\",\n",
    "    # Which observation filter to apply to the observation\n",
    "    \"observation_filter\": \"NoFilter\",\n",
    "    # Uses the sync samples optimizer instead of the multi-gpu one. This does\n",
    "    # not support minibatches.\n",
    "    \"simple_optimizer\": False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Train the RL model using the Python SDK Script mode\n",
    "\n",
    "If you are using local mode, the training will run on the notebook instance. When using SageMaker for training, you can select a GPU or CPU instance. The RLEstimator is used for training RL jobs. This code cell does the following:\n",
    "\n",
    "* Specifies the source directory where the environment, presets and training code is uploaded.\n",
    "* Specifies the entry point as the training code \n",
    "* Specifies the choice of RL toolkit and framework. This automatically resolves to the ECR path for the RL Container. \n",
    "* Defines the training parameters such as the instance count, job name, S3 path for output and job name. \n",
    "* Specifies the hyperparameters for the RL agent algorithm. The RLCOACH_PRESET or the RLRAY_PRESET can be used to specify the RL agent algorithm you want to use. \n",
    "* Defines the metrics definitions that you are interested in capturing in your logs. These can also be visualized in CloudWatch and SageMaker Notebooks. \n",
    "\n",
    "\n",
    "12. **Run** this code cell to begin training your RL model. It should take about 10-15 minutes for training to complete. Training time is capped at around 16 minutes as a time constraint for this workshop.\n",
    "\n",
    "**Tip:** You can also monitor the progress of your training job by going back to the Amazon SageMaker Management Console and find the **Training jobs** link in the left navigation pane.\n",
    "\n",
    "If you run into an error saying `ResourceLimitExceeded`, then change the EC2 instance type that training is happening on in **Step 4** above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = RLEstimator.default_metric_definitions(RLToolkit.RAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions.append({'Name': 'dist_from_center','Regex': 'dist_from_center=(.*?);'})\n",
    "metric_definitions.append({'Name': 'vel_at_end','Regex': 'vel_at_end=(.*?);'})\n",
    "metric_definitions.append({'Name': 'angle_at_end','Regex': 'angle_at_end=(.*?);'})\n",
    "metric_definitions.append({'Name': 'fuel_used','Regex': 'fuel_used=(.*?);'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceLimitExceeded",
     "evalue": "An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.p3.2xlarge for training job usage' is 2 Instances, with current utilization of 2 Instances and a request delta of 2 Instances. Please contact AWS support to request an increase for this limit.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_spot_checkpoint_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image, algorithm_arn, encrypt_inter_container_traffic, train_use_spot_instances, checkpoint_s3_uri, checkpoint_local_path)\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     def compile_model(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m: An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.p3.2xlarge for training job usage' is 2 Instances, with current utilization of 2 Instances and a request delta of 2 Instances. Please contact AWS support to request an increase for this limit."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "    \n",
    "estimator = RLEstimator(entry_point=trainscript,\n",
    "                        source_dir='src',\n",
    "                        dependencies=[\"common/sagemaker_rl\"],\n",
    "                        image_name=custom_image_name,\n",
    "                        role=role,\n",
    "                        train_instance_type=instance_type,\n",
    "                        train_instance_count=2,\n",
    "                        output_path=s3_output_path,\n",
    "                        base_job_name=job_name_prefix,\n",
    "                        metric_definitions=metric_definitions,\n",
    "                        train_max_run=250\n",
    "                    )\n",
    "\n",
    "estimator.fit(wait=False)\n",
    "job_name = estimator.latest_training_job.job_name\n",
    "print(\"Training job: %s\" % job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Visualization\n",
    "\n",
    "RL training can take a long time.  So while it's running there are a variety of ways we can track progress of the running training job.  Some intermediate output gets saved to S3 during training, so we'll set up to capture that. This code cell defines the path to where outputs are stored for specific training jobs. \n",
    "\n",
    "13. Always keep everything default in this code cell and **run** it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'job_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-e2b516c02e95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Job name: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ms3_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"s3://{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms3_bucket\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mintermediate_folder_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{}/output/intermediate/\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'job_name' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Job name: {}\".format(job_name))\n",
    "\n",
    "s3_url = \"s3://{}/{}\".format(s3_bucket,job_name)\n",
    "\n",
    "intermediate_folder_key = \"{}/output/intermediate/\".format(job_name)\n",
    "intermediate_url = \"s3://{}/{}\".format(s3_bucket, intermediate_folder_key)\n",
    "\n",
    "print(\"S3 job path: {}\".format(s3_url))\n",
    "print(\"Intermediate folder path: {}\".format(intermediate_url))\n",
    "    \n",
    "tmp_dir = \"/tmp/{}\".format(job_name)\n",
    "os.system(\"mkdir {}\".format(tmp_dir))\n",
    "print(\"Create local folder {}\".format(tmp_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch videos of training rollouts\n",
    "\n",
    "Videos of certain rollouts get written to S3 during agent training.  Here we fetch the last 10 videos from S3, and render the last one.\n",
    "\n",
    "14. **Run** the following code cells to get the most recent video outputs of your trained lunar lander agent. You will be able to display the video in the SageMaker notebook itself. Hit **Run interact** to play the latest video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_videos = wait_for_s3_object(\n",
    "            s3_bucket, intermediate_folder_key, tmp_dir, \n",
    "            fetch_only=(lambda obj: obj.key.endswith(\".mp4\") and obj.size>0), \n",
    "            limit=50, training_job_name=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls -l --block-size=M /tmp/{job_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, Video\n",
    "import ipywidgets as widgets\n",
    "video=0\n",
    "def showvideo(i):\n",
    "    last_video = sorted(recent_videos)[i]\n",
    "    return Video.from_file(last_video)\n",
    "print(len(recent_videos))\n",
    "\n",
    "video = interact_manual(showvideo, i=widgets.IntSlider(min=0,max=49,step=1,value=49));\n",
    "print(\"Does landing look better for higher values of i, i.e. later videos?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot metrics for training job\n",
    "\n",
    "We can see the reward metric of the training as it's running, using algorithm metrics that are recorded in Amazon CloudWatch metrics. We can plot this to see the performance of the model over time.\n",
    "\n",
    "15. **Run** the following code cell to view a plot of metrics from your training. \n",
    "\n",
    "16. You can also see the metrics in the **AWS Management Console** by finding your training job, clicking on it to expand details about it, and scrolling down to the Monitor section.\n",
    "\n",
    "    * Do to this, make sure you are on the SageMaker Management Console.\n",
    "    * Find the Training jobs link on the left navigation pane\n",
    "    * Find your latest training job and click it to see more details.\n",
    "    * Scroll down to the monitor section\n",
    "    * You will be able to see plotted metrics like episode reward mean, episode reward max, as well as other metrics like CPU and memory utilization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sagemaker.analytics import TrainingJobAnalytics\n",
    "\n",
    "if not local_mode:\n",
    "    df = TrainingJobAnalytics(job_name, ['episode_reward_mean']).dataframe()\n",
    "    num_metrics = len(df)\n",
    "    if num_metrics == 0:\n",
    "        print(\"No algorithm metrics found in CloudWatch\")\n",
    "    else:\n",
    "        plt = df.plot(x='timestamp', y='value', figsize=(12,5), legend=True, style='b-')\n",
    "        plt.set_ylabel('Mean reward per episode')\n",
    "        plt.set_xlabel('Training time (s)')\n",
    "else:\n",
    "    print(\"Can't plot metrics in local mode.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you!\n",
    "\n",
    "You have finished exploring the SageMaker notebook, including understanding how to set the scenario, tune hyperparameters, build docker containers, train your RL models, and visualize outputs. \n",
    "\n",
    "You can continue to use this notebook for training your agent in other scenarios by starting again at the top and setting the scenario appropriately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
